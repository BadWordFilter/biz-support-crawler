import requests
from bs4 import BeautifulSoup
import json
import datetime
import os

# í¬ë¡¤ë§í•  ëŒ€ìƒ URL (ì˜ˆì‹œ: K-Startup)
# ì‹¤ì œë¡œëŠ” ì—¬ëŸ¬ ì‚¬ì´íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ê´€ë¦¬í•˜ê±°ë‚˜, íŠ¹ì • í˜ì´ì§€ êµ¬ì¡°ì— ë§ì¶° íŒŒì‹±í•´ì•¼ í•©ë‹ˆë‹¤.
TARGET_URL = "https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do"
OUTPUT_FILE = "scripts/data.js"

def fetch_data():
    print("í¬ë¡¤ëŸ¬ ì‹œì‘: ìµœì‹  ê³µê³  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    
    # ê¹ƒí—ˆë¸Œ ì•¡ì…˜ í™˜ê²½ì—ì„œëŠ” requests ì‚¬ìš© ê°€ëŠ¥
    # ì§€ê¸ˆì€ ë°ëª¨ë¥¼ ìœ„í•´ ê³ ì •ëœ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì§€ë§Œ, 
    # ì‹¤ì œë¡œëŠ” ì•„ë˜ì™€ ê°™ì´ requests.get(url)ì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë©´ ë©ë‹ˆë‹¤.
    
    # response = requests.get(TARGET_URL)
    # soup = BeautifulSoup(response.text, 'html.parser')
    # ... íŒŒì‹± ë¡œì§ ...

    # ë°ëª¨ìš©: í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë™ì  ë°ì´í„° ìƒì„± (ìë™í™” ì‘ë™ í™•ì¸ìš©)
    today = datetime.date.today()
    
    # ê°€ìƒì˜ í¬ë¡¤ë§ ë°ì´í„°
    crawled_data = [
        {
            "id": "2026999",
            "title": f"ğŸ”” [ìë™ì—…ë°ì´íŠ¸] {today.strftime('%Y-%m-%d')} ê¸°ì¤€ ìµœì‹  ì°½ì—…ì§€ì› ê³µê³ ",
            "organization": "BizSupport Bot",
            "category": "support",
            "deadline": "2026-12-31",
            "dDay": "D-365",
            "views": 1,
            "tags": ["#ìë™ì—…ë°ì´íŠ¸", "#GithubActions", "#í¬ë¡¤ë§ì„±ê³µ"],
            "link": "https://github.com"
        },
        {
            "id": "2026001",
            "title": "2026ë…„ ì¤‘ì•™ë¶€ì²˜ ë° ì§€ìì²´ ì°½ì—…ì§€ì›ì‚¬ì—… í†µí•©ê³µê³ ",
            "organization": "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€",
            "category": "support",
            "deadline": "2026-12-31",
            "dDay": "D-365",
            "views": 15420,
            "tags": ["#2026ë…„", "#í†µí•©ê³µê³ ", "#K-Startup"],
            "link": "https://www.k-startup.go.kr/web/contents/bizpbanc-ongoing.do"
        },
        {
            "id": "2026002",
            "title": "2026ë…„ ì²­ë…„ì°½ì—…ì‚¬ê´€í•™êµ 16ê¸° ì…êµìƒ ëª¨ì§‘",
            "organization": "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ì§„í¥ê³µë‹¨",
            "category": "support",
            "deadline": "2026-02-12",
            "dDay": "D-43",
            "views": 8900,
            "tags": ["#ì²­ë…„ì°½ì—…", "#ì‚¬ê´€í•™êµ", "#ìµœëŒ€1ì–µ"],
            "link": "https://www.k-startup.go.kr/"
        },
        {
            "id": "2026006",
            "title": "ë„ì „! K-ìŠ¤íƒ€íŠ¸ì—… 2026 í˜ì‹ ì°½ì—…ë¦¬ê·¸ ì°¸ê°€ì ëª¨ì§‘",
            "organization": "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€",
            "category": "contest",
            "deadline": "2026-04-30",
            "dDay: "D-120",
            "views": 45000,
            "tags": ["#ê²½ì§„ëŒ€íšŒ", "#ìƒê¸ˆ", "#ëŒ€í†µë ¹ìƒ"],
            "link": "https://www.k-startup.go.kr/"
        }
    ]
    
    return crawled_data

def save_to_js(data):
    # ìë°”ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ì €ì¥
    js_content = f"const supportPrograms = {json.dumps(data, ensure_ascii=False, indent=4)};"
    
    # ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ utf-8 ì§€ì •
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(js_content)
    print(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE}")

if __name__ == "__main__":
    try:
        data = fetch_data()
        save_to_js(data)
        print("ì„±ê³µì ìœ¼ë¡œ í¬ë¡¤ë§ì„ ë§ˆì³¤ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit(1)
